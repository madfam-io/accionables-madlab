# Environment
NODE_ENV=development

# Server
PORT=3001
HOST=0.0.0.0

# Database (Docker Compose will use these defaults)
DATABASE_URL=postgresql://madlab:madlab@postgres:5432/madlab
POSTGRES_USER=madlab
POSTGRES_PASSWORD=madlab
POSTGRES_DB=madlab

# CORS (Comma-separated origins for production)
ALLOWED_ORIGINS=https://madlab.app,https://app.madlab.io

# Janua IdP Integration (Production only)
JANUA_ISSUER=https://auth.enclii.com
JANUA_AUDIENCE=madlab-api
JANUA_JWKS_URI=https://auth.enclii.com/.well-known/jwks.json

# =============================================================================
# AI Provider Configuration
# =============================================================================
# Choose ONE provider: ollama | groq | together | custom | mock (default)
AI_PROVIDER=mock

# -----------------------------------------------------------------------------
# Option 1: Ollama (Local - FREE, Private)
# -----------------------------------------------------------------------------
# Run open-source models locally. Requires Ollama installed.
# Install: https://ollama.ai
# Models: llama3.2, mistral, mixtral, qwen2.5, deepseek-coder, etc.
#
# AI_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2

# -----------------------------------------------------------------------------
# Option 2: Groq (API - FREE tier, Very Fast)
# -----------------------------------------------------------------------------
# Fastest inference for open models. Free tier: 30 req/min
# Get API key: https://console.groq.com
# Models: llama-3.1-70b-versatile, llama-3.1-8b-instant, mixtral-8x7b-32768
#
# AI_PROVIDER=groq
# GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxx
# GROQ_MODEL=llama-3.1-70b-versatile

# -----------------------------------------------------------------------------
# Option 3: Together.ai (API - Pay per token)
# -----------------------------------------------------------------------------
# Wide model selection, competitive pricing
# Get API key: https://api.together.xyz
# Models: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo, mistralai/Mixtral-8x7B-Instruct-v0.1
#
# AI_PROVIDER=together
# TOGETHER_API_KEY=xxxxxxxxxxxxxxxxxxxx
# TOGETHER_MODEL=meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo

# -----------------------------------------------------------------------------
# Option 4: Custom (BYOK - Any OpenAI-compatible endpoint)
# -----------------------------------------------------------------------------
# Use any OpenAI-compatible API (vLLM, LocalAI, LM Studio, etc.)
#
# AI_PROVIDER=custom
# AI_BASE_URL=http://localhost:8000/v1
# AI_API_KEY=your-api-key-if-needed
# AI_MODEL=your-model-name
